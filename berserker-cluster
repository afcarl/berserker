#!/bin/bash

# Convenience script to run Berserker on Apache Spark cluster
# Asssume you have Apache Spark installed and configured locally

E_NOT_BUILT=140
E_NO_SPARK=141

jar="target/scala-2.11/berserker_2.11-0.0.2.jar"
build_command="./sbt package"

siva="$GOPATH/bin/siva"
siva_build_command="go get -u gopkg.in/src-d/go-siva.v1/..."

hash java >/dev/null 2>&1 || { echo "Please install Java" >&2; exit 1; }

if [[ ! -f "${jar}" ]]; then
    echo "${jar} not found. Running build ${build_command}"
    $build_command
fi

if [[ ! -f "${siva}" ]]; then
    echo "go-siva binary not found in ${siva}. Installing it using ${siva_build_command}"
    hash go >/dev/null 2>&1 || { echo "Please install Golang" >&2; exit 1; }
    $siva_build_command
fi



sparkSubmit() {
    if hash spark-submit 2>/dev/null; then
        exec spark-submit "$@"
    elif [[ -n "${SPARK_HOME}" ]]; then
        echo "Using spark-submit from ${SPARK_HOME}"
        "${SPARK_HOME}/bin/spark-submit" "$@"
    else
        echo "Please, install and configure Apache Spark and set SPARK_HOME"
        exit "${E_NO_SPARK}"
    fi
}


# --exclude-packages works around https://bugs.eclipse.org/bugs/show_bug.cgi?id=514326
sparkSubmit \
  --class tech.sourced.berserker.SparkDriver \
  --master "${MASTER:=yarn}" \
  --name "Berserker (UAST extractor)" \
  --conf "spark.driver.userClassPathFirst=true" \
  --conf "spark.executor.userClassPathFirst=true" \
  --exclude-packages "org.slf4j:slf4j-api" \
  --repositories "https://jitpack.io" \
  --packages "com.github.src-d:enry-java:1.0,tech.sourced:siva-java_2.11:0.1.0-SNAPSHOT,org.eclipse.jgit:org.eclipse.jgit:4.8.0.201706111038-r,org.rogach:scallop_2.11:3.0.3,io.grpc:grpc-netty:1.4.0,com.trueaccord.scalapb:scalapb-runtime-grpc_2.11:0.6.0,com.github.bzz:client-scala:390362d7dc" \
  "${PWD}/${jar}" \
  "$@"
